{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wallybeamm/neural_entity_recognition/blob/feature%2Finit/enitity_recognation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR7VtBNNDxGG"
      },
      "source": [
        "# Entity Recognotion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnfhCMLID-DH"
      },
      "source": [
        "## Academic Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGcV6occEIyo"
      },
      "source": [
        "## Practical Implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKM_js_kETwh"
      },
      "source": [
        "### Set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q5-rtsLESnQ"
      },
      "outputs": [],
      "source": [
        "json_path = '/content/train.jsonl'\n",
        "model_name = 'en_core_web_trf'\n",
        "\n",
        "output_dir = \"/content/Model\"\n",
        "n_iter = 100\n",
        "learn_rate=2e-5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktarw1Z6LJAm"
      },
      "source": [
        "### Install missing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqhj9L9rMQxF"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_trf\n",
        "!pip3 install -qU wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6ZqS2L_lOQO"
      },
      "source": [
        "### Import librariers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Sf6bnwqflSFU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import wandb\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "import json\n",
        "import random\n",
        "from spacy.training.example import Example\n",
        "import thinc\n",
        "import torch\n",
        "from spacy.util import minibatch\n",
        "from tqdm.auto import tqdm\n",
        "import unicodedata\n",
        "import wasabi\n",
        "import numpy\n",
        "from collections import Counter\n",
        "import gc \n",
        "from spacy.scorer import Scorer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77Q66GdBJQLF"
      },
      "source": [
        "### Initiliaze Loggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNVh0fH1Kc30"
      },
      "outputs": [],
      "source": [
        "wandb.login()\n",
        "# W&B Artifact naming convention: `wandb_entity/wandb_project/artifact_name:version`\n",
        "spacy_artifact = 'wandb/spacy/spacy_demo:v3'\n",
        "\n",
        "# Our output directory name\n",
        "spacy_dir = Path(\"my_spacy_demo\") \n",
        "\n",
        "with wandb.init(project='spacy_demo') as run: # \"config\" is optional here\n",
        "    artifact = run.use_artifact(spacy_artifact)\n",
        "    _ = artifact.download(spacy_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhEXrsfhJweP"
      },
      "source": [
        "##Train the model by using Spacy's function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KY6LalqJcuf"
      },
      "source": [
        "### Reformat dataset for Spacy's train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d97cStU_jrMK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_dataset(path):\n",
        "    data = []\n",
        "    for line in open(path, 'r', encoding=\"utf-8\"):\n",
        "        line_dict = json.loads(line)\n",
        "        data.append((line_dict['data'].replace('\\n', ' '), line_dict['label']))\n",
        "    return data\n",
        "\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "training_data = load_dataset('./train.jsonl')\n",
        "# the DocBin will store the example documents\n",
        "db = DocBin()\n",
        "for text, annotations in training_data:\n",
        "    doc = nlp(text)\n",
        "    ents = []\n",
        "    #print(annotations)\n",
        "    for start, end, label in annotations:\n",
        "        if label == '' or label == None:\n",
        "          continue\n",
        "\n",
        "        span = doc.char_span(start, end, label=label)\n",
        "        if span == None:\n",
        "          continue\n",
        "        print(span)\n",
        "        ents.append(span)\n",
        "    #print(ents)\n",
        "    \n",
        "    doc.ents = ents\n",
        "    db.add(doc)\n",
        "db.to_disk(\"./train.spacy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkRGswwcJkEj"
      },
      "source": [
        "### Create the config file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib0jJiTBlZUw",
        "outputId": "0f58a4bc-974e-4148-e747-01753635b532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2mâœ” Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2mâœ” Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZdMukmIJpge"
      },
      "source": [
        "### Investigate the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khOArg7OmVqx",
        "outputId": "5d997a4e-1693-45da-c4d1-57aa66bc6de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "Downloading config.json: 100% 481/481 [00:00<00:00, 445kB/s]\n",
            "Downloading vocab.json: 100% 878k/878k [00:00<00:00, 5.12MB/s]\n",
            "Downloading merges.txt: 100% 446k/446k [00:00<00:00, 3.12MB/s]\n",
            "Downloading tokenizer.json: 100% 1.29M/1.29M [00:00<00:00, 6.39MB/s]\n",
            "Downloading pytorch_model.bin: 100% 478M/478M [00:06<00:00, 75.0MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\u001b[38;5;2mâœ” Pipeline can be initialized with data\u001b[0m\n",
            "\u001b[38;5;2mâœ” Corpus is loadable\u001b[0m\n",
            "\u001b[1m\n",
            "=============================== Training stats ===============================\u001b[0m\n",
            "Language: en\n",
            "Training pipeline: transformer, ner\n",
            "170 training docs\n",
            "51 evaluation docs\n",
            "\u001b[38;5;2mâœ” No overlap between training and evaluation data\u001b[0m\n",
            "\u001b[38;5;3mâš  Low number of examples to train a new pipeline (170)\u001b[0m\n",
            "\u001b[1m\n",
            "============================== Vocab & Vectors ==============================\u001b[0m\n",
            "\u001b[38;5;4mâ„¹ 333507 total word(s) in the data (31415 unique)\u001b[0m\n",
            "\u001b[38;5;4mâ„¹ No word vectors present in the package\u001b[0m\n",
            "\u001b[1m\n",
            "========================== Named Entity Recognition ==========================\u001b[0m\n",
            "\u001b[38;5;4mâ„¹ 13 label(s)\u001b[0m\n",
            "0 missing value(s) (tokens with '-' label)\n",
            "\u001b[38;5;3mâš  Low number of examples for label 'ART_ADVISOR' (7)\u001b[0m\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[2K\u001b[38;5;2mâœ” Examples without occurrences available for all labels\u001b[0m\n",
            "\u001b[38;5;2mâœ” No entities consisting of or starting/ending with whitespace\u001b[0m\n",
            "\u001b[38;5;2mâœ” No entities crossing sentence boundaries\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Summary ==================================\u001b[0m\n",
            "\u001b[38;5;2mâœ” 6 checks passed\u001b[0m\n",
            "\u001b[38;5;3mâš  2 warnings\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy debug data config.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zD7Uz5rR1eN"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYuTchwLfA2j",
        "outputId": "9d591724-4421-4071-e63e-2c15c300e7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2mâœ” Created output directory: my_spacy_demo/training/cnn\u001b[0m\n",
            "\u001b[38;5;4mâ„¹ Saving to output directory: my_spacy_demo/training/cnn\u001b[0m\n",
            "\u001b[38;5;4mâ„¹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-10-22 19:36:50,967] [INFO] Set up nlp object from config\n",
            "INFO:spacy:Set up nlp object from config\n",
            "[2022-10-22 19:36:51,748] [INFO] Pipeline: ['transformer', 'ner']\n",
            "INFO:spacy:Pipeline: ['transformer', 'ner']\n",
            "[2022-10-22 19:36:51,753] [INFO] Created vocabulary\n",
            "INFO:spacy:Created vocabulary\n",
            "[2022-10-22 19:36:51,755] [INFO] Finished initializing nlp object\n",
            "INFO:spacy:Finished initializing nlp object\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2022-10-22 19:37:22,918] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "INFO:spacy:Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2mâœ” Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mâ„¹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mâ„¹ Initial learn rate: 0.0\u001b[0m\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwallybeam\u001b[0m (\u001b[33mwallybeamm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20221022_193723-1mnk1u8q\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdainty-thunder-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/wallybeamm/spacy_demo\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/wallybeamm/spacy_demo/runs/1mnk1u8q\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./my_spacy_demo/corpus)... Done. 0.1s\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (769 > 512). Running this sequence through the model will result in indexing errors\n",
            "  0       0        1292.59   2474.66    0.04    0.02    0.20    0.00\n",
            "  2     200      231389.36  106741.24   48.95   47.78   50.19    0.49\n",
            "  5     400      206328.86  38619.35   68.73   63.41   75.02    0.69\n",
            "  7     600       33627.02  24846.84   69.83   68.32   71.40    0.70\n",
            " 10     800        7641.55  16386.97   72.60   68.88   76.75    0.73\n",
            " 12    1000        5947.79  13647.89   73.71   73.38   74.04    0.74\n",
            " 15    1200        5971.06  11793.82   74.90   71.09   79.14    0.75\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./my_spacy_demo/training/cnn/model-last)... "
          ]
        }
      ],
      "source": [
        "!python -m spacy train config.cfg -o my_spacy_demo/training/cnn --gpu-id 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzHdKvdHJ_UG"
      },
      "source": [
        "## Define our functions for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o4_B-PDEtYF"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "\n",
        "def load_dataset(path):\n",
        "\n",
        "  data = []\n",
        "  for line in open(path, 'r'):\n",
        "      line_dict = json.loads(line)\n",
        "      data.append((line_dict['data'].replace('\\n', ' '), line_dict['label']))\n",
        "  return data\n",
        "\n",
        "# Display entity info\n",
        "def show_ents(doc): \n",
        "  spacy.displacy.render(doc, style=\"ent\", jupyter=True) # if from notebook else displacy.serve(doc, style=\"ent\") generally\n",
        "\n",
        "def cyclic_triangular_rate(min_lr, max_lr, period):\n",
        "    it = 1\n",
        "    while True:\n",
        "        # https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee\n",
        "        cycle = numpy.floor(1 + it / (2 * period))\n",
        "        x = numpy.abs(it / period - 2 * cycle + 1)\n",
        "        relative = max(0, 1 - x)\n",
        "        yield min_lr + (max_lr - min_lr) * relative\n",
        "        it += 1\n",
        "\n",
        "def train(data, model):\n",
        "  # Main\n",
        "  from thinc.api import set_gpu_allocator, require_gpu\n",
        "\n",
        "  # Default scoring pipeline\n",
        "  scorer = Scorer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Use the GPU, with memory allocations directed via PyTorch.\n",
        "  # This prevents out-of-memory errors that would otherwise occur from competing\n",
        "  # memory pools.\n",
        "\n",
        "  set_gpu_allocator(\"pytorch\")\n",
        "  if \"ner\" not in model.pipe_names:\n",
        "      ner = model.create_pipe(\"ner\") # \"architecture\": \"ensemble\" simple_cnn ensemble, bow # https://spacy.io/api/annotation\n",
        "      model.add_pipe(ner)\n",
        "  else:\n",
        "      ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "  # Update the label list\n",
        "  for annotations in data:\n",
        "      for ent in annotations[1]:\n",
        "          ner.add_label(ent[2])\n",
        "\n",
        "  learn_rates = cyclic_triangular_rate(\n",
        "    learn_rate / 3, learn_rate * 3, 2 * len(train_data) // 1\n",
        "    )\n",
        "\n",
        "  with model.select_pipes(enable=['ner', 'transformer']):  # only train NER\n",
        "      optimizer = model.resume_training()\n",
        "      i = 0\n",
        "      for itn in range(n_iter):\n",
        "        \n",
        "          random.shuffle(train_data)\n",
        "          losses = {}\n",
        "          batches = spacy.util.minibatch(train_data, size=8)\n",
        "          for batch in batches:\n",
        "              for text, annotations in batch:\n",
        "                  print(text)\n",
        "                  print(annotations)\n",
        "                  # create Example \n",
        "                  #cupy.get_default_memory_pool().free_all_blocks()              \n",
        "                  doc = model.make_doc(text)\n",
        "                  annotations = {'entities' : annotations}\n",
        "                  example = Example.from_dict(doc, annotations)\n",
        "                  # try to visualize the content of the example\n",
        "\n",
        "                  # Update the model\n",
        "                  #print('Example')\n",
        "                  #print(example)\n",
        "                  #print('doc')\n",
        "                  #print(doc)\n",
        "                  #print(len(doc))\n",
        "                  #print('annotations')\n",
        "                  #print(annotations)\n",
        "                  #print(len(annotations))\n",
        "                  # 100 Mbi Gpu/Memory\n",
        "                  \n",
        "\n",
        "                  #i = i + 1\n",
        "                  #print(i)\n",
        "                  model.update([example], sgd=optimizer, drop=0.1, losses=losses ) # Be sure that you are defining batch size\n",
        "                  #if output_dir is not None:\n",
        "                  #  model.to_disk(output_dir)\n",
        "                  #  print(\"Saved model to\", output_dir)\n",
        "                  #torch.cuda.empty_cache()\n",
        "                  #gc.collect()\n",
        "                  #torch.cuda.empty_cache()\n",
        "                  #del model\n",
        "                  #model = spacy.load(output_dir)\n",
        "\n",
        "              scorer = Scorer(model)\n",
        "              scores = scorer.score([example])\n",
        "              print(scores)\n",
        "\n",
        "                  \n",
        "\n",
        "  return model\n",
        "\n",
        "def split(data, train_percantage):\n",
        "  # Split the data\n",
        "  train_lenght = int(len(data)*train_percantage)\n",
        "  train_data = data[:train_lenght]\n",
        "  test_data = data[train_lenght:]\n",
        "  return train_data, test_data\n",
        "\n",
        "def test(test_data, model):\n",
        "  for text, _ in test_data:\n",
        "    doc = nlp(text)\n",
        "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "def save_model(model, output_dir):\n",
        "  if output_dir is not None:\n",
        "      nlp.to_disk(output_dir)\n",
        "      print(\"Saved model to\", output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkyIds8nlhPp"
      },
      "outputs": [],
      "source": [
        "# Main\n",
        "from thinc.api import set_gpu_allocator, require_gpu\n",
        "\n",
        "# Use the GPU, with memory allocations directed via PyTorch.\n",
        "# This prevents out-of-memory errors that would otherwise occur from competing\n",
        "# memory pools.#\n",
        "set_gpu_allocator(\"pytorch\")\n",
        "require_gpu(0)\n",
        "data = load_dataset(json_path)\n",
        "\n",
        "nlp = spacy.load(model_name)\n",
        "\n",
        "train_data, test_data = split(data, 1)\n",
        "#nlp.max_length = 100000\n",
        "#nlp.max_split_size_mb = 100\n",
        "finetuned_model = train(train_data, nlp)\n",
        "\n",
        "if output_dir is not None:\n",
        "    finetuned_model.to_disk(output_dir)\n",
        "    print(\"Saved model to\", output_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPdsjmFUlhKCyu/gEQhDbHg",
      "collapsed_sections": [
        "xkRGswwcJkEj"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
